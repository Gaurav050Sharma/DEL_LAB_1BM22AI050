import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np

# Generate a synthetic dataset
np.random.seed(42)
X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_classes=2)
scaler = StandardScaler()
X = scaler.fit_transform(X)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
X_val = torch.tensor(X_val, dtype=torch.float32)
y_val = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)

# Define the Deep Neural Network
class DNN(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(DNN, self).__init__()
        layers = []
        in_size = input_size
        for hidden_size in hidden_sizes:
            layers.append(nn.Linear(in_size, hidden_size))
            layers.append(nn.ReLU())
            in_size = hidden_size
        layers.append(nn.Linear(in_size, output_size))
        layers.append(nn.Sigmoid())
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return self.network(x)

# Initialize the network
input_size = X_train.shape[1]
hidden_sizes = [64, 32, 16]
output_size = 1
model = DNN(input_size, hidden_sizes, output_size)

# Training function
def train_model(optimizer_type="gradient_descent", learning_rate=0.01, epochs=100, batch_size=None):
    model.apply(lambda layer: layer.reset_parameters() if hasattr(layer, 'reset_parameters') else None)  # Reset weights

    if optimizer_type == "gradient_descent":
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)  # Full batch Gradient Descent
    elif optimizer_type == "stochastic_gradient_descent":
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)  # SGD (mini-batch or stochastic)
    else:
        raise ValueError("Unsupported optimizer type.")

    criterion = nn.BCELoss()  # Binary Cross-Entropy Loss

    train_losses, val_losses = [], []

    for epoch in range(epochs):
        model.train()

        if batch_size:  # SGD with mini-batches
            perm = torch.randperm(X_train.size(0))
            for i in range(0, X_train.size(0), batch_size):
                indices = perm[i:i + batch_size]
                batch_X, batch_y = X_train[indices], y_train[indices]
                
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
        else:  # Full-batch Gradient Descent
            optimizer.zero_grad()
            outputs = model(X_train)
            loss = criterion(outputs, y_train)
            loss.backward()
            optimizer.step()

        # Evaluate on validation set
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val)
            val_loss = criterion(val_outputs, y_val)

        train_losses.append(loss.item())
        val_losses.append(val_loss.item())

        print(
            f"Epoch {epoch + 1}/{epochs} | "
            f"Train Loss: {loss.item():.4f} | Val Loss: {val_loss.item():.4f}"
        )

    return train_losses, val_losses

# Train with Gradient Descent
print("\nTraining with Gradient Descent:")
gd_train_losses, gd_val_losses = train_model(optimizer_type="gradient_descent", learning_rate=0.01, epochs=50)

# Train with Stochastic Gradient Descent (mini-batches)
print("\nTraining with Stochastic Gradient Descent (mini-batches):")
sgd_train_losses, sgd_val_losses = train_model(optimizer_type="stochastic_gradient_descent", learning_rate=0.01, epochs=50, batch_size=32)

# Plot training and validation losses
plt.figure(figsize=(10, 6))
plt.plot(gd_train_losses, label='Gradient Descent - Train Loss')
plt.plot(gd_val_losses, label='Gradient Descent - Val Loss')
plt.plot(sgd_train_losses, label='SGD - Train Loss', linestyle='dashed')
plt.plot(sgd_val_losses, label='SGD - Val Loss', linestyle='dashed')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Loss Comparison: Gradient Descent vs Stochastic Gradient Descent')
plt.legend()
plt.grid()
plt.show()
