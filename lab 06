import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# Define the neural network
class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return self.sigmoid(x)

# Adversarial example generation
def generate_adversarial_example(model, inputs, labels, epsilon=0.1):
    inputs.requires_grad = True
    outputs = model(inputs)
    loss = nn.BCELoss()(outputs, labels)
    model.zero_grad()
    loss.backward()
    gradients = inputs.grad.data.sign()
    adversarial_inputs = inputs + epsilon * gradients
    return torch.clamp(adversarial_inputs, 0, 1)

# Tangent Prop regularizer
def tangent_prop_regularizer(model, inputs, epsilon=0.1):
    tangent_directions = torch.randn_like(inputs) * epsilon
    perturbed_inputs = inputs + tangent_directions
    original_outputs = model(inputs)
    perturbed_outputs = model(perturbed_inputs)
    return ((original_outputs - perturbed_outputs) ** 2).mean()

# Tangent Distance
def tangent_distance(X1, X2, tangent_vectors):
    differences = X1 - X2
    distances = torch.norm(differences - tangent_vectors, dim=1)
    return distances

# Tangent Classifier
class TangentClassifier:
    def __init__(self):
        pass

    def fit(self, X, y, tangent_vectors):
        self.tangent_vectors = tangent_vectors
        self.X = X
        self.y = y

    def predict(self, X_test):
        predictions = []
        for x in X_test:
            distances = tangent_distance(self.X, x.unsqueeze(0), self.tangent_vectors)
            nearest_idx = torch.argmin(distances)
            predictions.append(self.y[nearest_idx])
        return torch.tensor(predictions)

# Training and evaluation
def train_model():
    # Simulate dataset
    np.random.seed(42)
    X = torch.tensor(np.random.rand(500, 2).astype(np.float32) * 10)
    y = (X[:, 0] + X[:, 1] > 10).float().unsqueeze(1)

    # Split into train and validation sets
    train_size = int(0.8 * len(X))
    X_train, X_val = X[:train_size], X[train_size:]
    y_train, y_val = y[:train_size], y[train_size:]

    # Model setup
    model = SimpleNN(input_size=2, hidden_size=10)
    optimizer = optim.Adam(model.parameters(), lr=0.01)
    criterion = nn.BCELoss()

    # Training loop
    epochs = 50
    epsilon = 0.1
    for epoch in range(epochs):
        model.train()

        # Generate adversarial examples
        X_adv = generate_adversarial_example(model, X_train.clone(), y_train, epsilon)

        # Forward pass with original and adversarial examples
        outputs = model(X_train)
        adv_outputs = model(X_adv)

        # Loss with tangent prop regularizer
        loss = criterion(outputs, y_train) + criterion(adv_outputs, y_train)
        loss += tangent_prop_regularizer(model, X_train, epsilon)

        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Validation
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val)
            val_loss = criterion(val_outputs, y_val).item()
        print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}")

    return model, X, y

# Train the model and evaluate the tangent classifier
model, X, y = train_model()

# Generate tangent vectors
tangent_vectors = torch.randn_like(X) * 0.1

# Fit tangent classifier
classifier = TangentClassifier()
classifier.fit(X, y, tangent_vectors)

# Test tangent classifier
X_test = torch.tensor(np.random.rand(10, 2).astype(np.float32) * 10)
predictions = classifier.predict(X_test)
print("Tangent Classifier Predictions:", predictions)
